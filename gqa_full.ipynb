{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vieira on GQA (FULL)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vieira-artifact/vieira-artifact-aaai24/blob/main/gqa_full.ipynb)\n",
        "\n",
        "In this notebook we explore using Vieira to solve data-points in the GQA visual question answer dataset.\n",
        "We will be using GPT-4 as the language model for semantically parsing the natural question.\n",
        "In order to save cost, we only demonstrate running on a single data-point in this notebook.\n",
        "Please feel free to insert your own API-key to run on more data-points.\n",
        "\n",
        "We also provide another notebook which uses the cached GPT-4 semantic parsing programs (named `gqa_main.ipynb`).\n",
        "There, you can fully reproduce our result without making extra calls to the GPT-4, which could be costly."
      ],
      "metadata": {
        "id": "KKARS4k72kYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking python version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUiqrgQATHsE",
        "outputId": "21a0b7f3-074b-46c4-b808-57bc5df24703"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download our mini dataset\n",
        "\n",
        "The dataset contains 500 samples"
      ],
      "metadata": {
        "id": "-rH8L6y_3Tvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/dataset/gqa_mini_data.zip\n",
        "!unzip gqa_mini_data.zip"
      ],
      "metadata": {
        "id": "D8uHW4eBconK",
        "outputId": "04e511f9-8a8e-4b1a-c688-8b6ddc3ed4d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-07 03:36:55--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/dataset/gqa_mini_data.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/6a7b82ae-ee15-4dc1-b119-8cf57a9ab7c8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033655Z&X-Amz-Expires=300&X-Amz-Signature=c02123283fa210939396f16ad929198f4939fe0222b39a4808cc65d6d3d737f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dgqa_mini_data.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:36:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/6a7b82ae-ee15-4dc1-b119-8cf57a9ab7c8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033655Z&X-Amz-Expires=300&X-Amz-Signature=c02123283fa210939396f16ad929198f4939fe0222b39a4808cc65d6d3d737f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dgqa_mini_data.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72014468 (69M) [application/octet-stream]\n",
            "Saving to: ‘gqa_mini_data.zip’\n",
            "\n",
            "gqa_mini_data.zip   100%[===================>]  68.68M   230MB/s    in 0.3s    \n",
            "\n",
            "2023-11-07 03:36:55 (230 MB/s) - ‘gqa_mini_data.zip’ saved [72014468/72014468]\n",
            "\n",
            "Archive:  gqa_mini_data.zip\n",
            "   creating: gqa_mini_data/\n",
            "  inflating: gqa_mini_data/mini_question_no_crop.pkl  \n",
            "  inflating: gqa_mini_data/.DS_Store  \n",
            "  inflating: __MACOSX/gqa_mini_data/._.DS_Store  \n",
            "   creating: gqa_mini_data/images/\n",
            "  inflating: gqa_mini_data/images/2358656.jpg  \n",
            "  inflating: gqa_mini_data/images/2404366.jpg  \n",
            "  inflating: gqa_mini_data/images/2354291.jpg  \n",
            "  inflating: gqa_mini_data/images/2393916.jpg  \n",
            "  inflating: gqa_mini_data/images/2336366.jpg  \n",
            "  inflating: gqa_mini_data/images/713177.jpg  \n",
            "  inflating: gqa_mini_data/images/2383250.jpg  \n",
            "  inflating: gqa_mini_data/images/2404945.jpg  \n",
            "  inflating: gqa_mini_data/images/2398738.jpg  \n",
            "  inflating: gqa_mini_data/images/2344647.jpg  \n",
            "  inflating: gqa_mini_data/images/2368391.jpg  \n",
            "  inflating: gqa_mini_data/images/2341298.jpg  \n",
            "  inflating: gqa_mini_data/images/2345940.jpg  \n",
            "  inflating: gqa_mini_data/images/2376391.jpg  \n",
            "  inflating: gqa_mini_data/images/1592734.jpg  \n",
            "  inflating: gqa_mini_data/images/2327691.jpg  \n",
            "  inflating: gqa_mini_data/images/2381718.jpg  \n",
            "  inflating: gqa_mini_data/images/2409079.jpg  \n",
            "  inflating: gqa_mini_data/images/2319964.jpg  \n",
            "  inflating: gqa_mini_data/images/2366154.jpg  \n",
            "  inflating: gqa_mini_data/images/2373559.jpg  \n",
            "  inflating: gqa_mini_data/images/2416824.jpg  \n",
            "  inflating: gqa_mini_data/images/2374975.jpg  \n",
            "  inflating: gqa_mini_data/images/2348054.jpg  \n",
            "  inflating: gqa_mini_data/images/2394117.jpg  \n",
            "  inflating: gqa_mini_data/images/2349558.jpg  \n",
            "  inflating: gqa_mini_data/images/2397542.jpg  \n",
            "  inflating: gqa_mini_data/images/2319409.jpg  \n",
            "  inflating: gqa_mini_data/images/2379464.jpg  \n",
            "  inflating: gqa_mini_data/images/2368780.jpg  \n",
            "  inflating: gqa_mini_data/images/1592655.jpg  \n",
            "  inflating: gqa_mini_data/images/2406558.jpg  \n",
            "  inflating: gqa_mini_data/images/2369449.jpg  \n",
            "  inflating: gqa_mini_data/images/4564.jpg  \n",
            "  inflating: gqa_mini_data/images/2351840.jpg  \n",
            "  inflating: gqa_mini_data/images/2415297.jpg  \n",
            "  inflating: gqa_mini_data/images/2361995.jpg  \n",
            "  inflating: gqa_mini_data/images/2367314.jpg  \n",
            "  inflating: gqa_mini_data/images/1160187.jpg  \n",
            "  inflating: gqa_mini_data/images/2363358.jpg  \n",
            "  inflating: gqa_mini_data/images/2316937.jpg  \n",
            "  inflating: gqa_mini_data/images/2382830.jpg  \n",
            "  inflating: gqa_mini_data/images/2350023.jpg  \n",
            "  inflating: gqa_mini_data/images/2340230.jpg  \n",
            "  inflating: gqa_mini_data/images/2398101.jpg  \n",
            "  inflating: gqa_mini_data/images/2383496.jpg  \n",
            "  inflating: gqa_mini_data/images/2352973.jpg  \n",
            "  inflating: gqa_mini_data/images/2368621.jpg  \n",
            "  inflating: gqa_mini_data/images/2342779.jpg  \n",
            "  inflating: gqa_mini_data/images/2393451.jpg  \n",
            "  inflating: gqa_mini_data/images/2404576.jpg  \n",
            "  inflating: gqa_mini_data/images/2381041.jpg  \n",
            "  inflating: gqa_mini_data/images/2377099.jpg  \n",
            "  inflating: gqa_mini_data/images/2377933.jpg  \n",
            "  inflating: gqa_mini_data/images/2411806.jpg  \n",
            "  inflating: gqa_mini_data/images/2402311.jpg  \n",
            "  inflating: gqa_mini_data/images/2404760.jpg  \n",
            "  inflating: gqa_mini_data/images/2406605.jpg  \n",
            "  inflating: gqa_mini_data/images/2355774.jpg  \n",
            "  inflating: gqa_mini_data/images/498061.jpg  \n",
            "  inflating: gqa_mini_data/images/2384461.jpg  \n",
            "  inflating: gqa_mini_data/images/2411423.jpg  \n",
            "  inflating: gqa_mini_data/images/2372334.jpg  \n",
            "  inflating: gqa_mini_data/images/2392363.jpg  \n",
            "  inflating: gqa_mini_data/images/2380798.jpg  \n",
            "  inflating: gqa_mini_data/images/2319393.jpg  \n",
            "  inflating: gqa_mini_data/images/4003.jpg  \n",
            "  inflating: gqa_mini_data/images/2342624.jpg  \n",
            "  inflating: gqa_mini_data/images/2373412.jpg  \n",
            "  inflating: gqa_mini_data/images/49.jpg  \n",
            "  inflating: gqa_mini_data/images/2377660.jpg  \n",
            "  inflating: gqa_mini_data/images/2365506.jpg  \n",
            "  inflating: gqa_mini_data/images/2321378.jpg  \n",
            "  inflating: gqa_mini_data/images/2385357.jpg  \n",
            "  inflating: gqa_mini_data/images/2355374.jpg  \n",
            "  inflating: gqa_mini_data/images/2373600.jpg  \n",
            "  inflating: gqa_mini_data/images/2365076.jpg  \n",
            "  inflating: gqa_mini_data/images/2327523.jpg  \n",
            "  inflating: gqa_mini_data/images/2374147.jpg  \n",
            "  inflating: gqa_mini_data/images/2395420.jpg  \n",
            "  inflating: gqa_mini_data/images/2346888.jpg  \n",
            "  inflating: gqa_mini_data/images/2413436.jpg  \n",
            "  inflating: gqa_mini_data/images/2341739.jpg  \n",
            "  inflating: gqa_mini_data/images/713207.jpg  \n",
            "  inflating: gqa_mini_data/images/2333146.jpg  \n",
            "  inflating: gqa_mini_data/images/2336002.jpg  \n",
            "  inflating: gqa_mini_data/images/2383652.jpg  \n",
            "  inflating: gqa_mini_data/images/2345749.jpg  \n",
            "  inflating: gqa_mini_data/images/2342998.jpg  \n",
            "  inflating: gqa_mini_data/images/2416836.jpg  \n",
            "  inflating: gqa_mini_data/images/2320113.jpg  \n",
            "  inflating: gqa_mini_data/images/2335091.jpg  \n",
            "  inflating: gqa_mini_data/images/2357827.jpg  \n",
            "  inflating: gqa_mini_data/images/2265.jpg  \n",
            "  inflating: gqa_mini_data/images/2324003.jpg  \n",
            "  inflating: gqa_mini_data/images/2377315.jpg  \n",
            "  inflating: gqa_mini_data/images/2367112.jpg  \n",
            "  inflating: gqa_mini_data/images/2327913.jpg  \n",
            "  inflating: gqa_mini_data/images/2400113.jpg  \n",
            "  inflating: gqa_mini_data/images/2341314.jpg  \n",
            "  inflating: gqa_mini_data/images/2407668.jpg  \n",
            "  inflating: gqa_mini_data/images/2362256.jpg  \n",
            "  inflating: gqa_mini_data/images/2401233.jpg  \n",
            "  inflating: gqa_mini_data/images/2399755.jpg  \n",
            "  inflating: gqa_mini_data/images/2392614.jpg  \n",
            "  inflating: gqa_mini_data/images/2380990.jpg  \n",
            "  inflating: gqa_mini_data/images/2391123.jpg  \n",
            "  inflating: gqa_mini_data/images/2330128.jpg  \n",
            "  inflating: gqa_mini_data/images/2318840.jpg  \n",
            "  inflating: gqa_mini_data/images/1160234.jpg  \n",
            "  inflating: gqa_mini_data/images/713210.jpg  \n",
            "  inflating: gqa_mini_data/images/2333179.jpg  \n",
            "  inflating: gqa_mini_data/images/2351059.jpg  \n",
            "  inflating: gqa_mini_data/images/2316312.jpg  \n",
            "  inflating: gqa_mini_data/images/2390163.jpg  \n",
            "  inflating: gqa_mini_data/images/2356045.jpg  \n",
            "  inflating: gqa_mini_data/images/2353897.jpg  \n",
            "  inflating: gqa_mini_data/images/2385745.jpg  \n",
            "  inflating: gqa_mini_data/images/2335051.jpg  \n",
            "  inflating: gqa_mini_data/images/2366409.jpg  \n",
            "  inflating: gqa_mini_data/images/2326818.jpg  \n",
            "  inflating: gqa_mini_data/images/2364395.jpg  \n",
            "  inflating: gqa_mini_data/images/2345373.jpg  \n",
            "  inflating: gqa_mini_data/images/1592334.jpg  \n",
            "  inflating: gqa_mini_data/images/2333597.jpg  \n",
            "  inflating: gqa_mini_data/images/2412716.jpg  \n",
            "  inflating: gqa_mini_data/images/2348870.jpg  \n",
            "  inflating: gqa_mini_data/images/2408202.jpg  \n",
            "  inflating: gqa_mini_data/images/2364829.jpg  \n",
            "  inflating: gqa_mini_data/images/2343514.jpg  \n",
            "  inflating: gqa_mini_data/images/2405083.jpg  \n",
            "  inflating: gqa_mini_data/images/2373329.jpg  \n",
            "  inflating: gqa_mini_data/images/2390930.jpg  \n",
            "  inflating: gqa_mini_data/images/2411720.jpg  \n",
            "  inflating: gqa_mini_data/images/2319669.jpg  \n",
            "  inflating: gqa_mini_data/images/2354543.jpg  \n",
            "  inflating: gqa_mini_data/images/2343017.jpg  \n",
            "  inflating: gqa_mini_data/images/2368286.jpg  \n",
            "  inflating: gqa_mini_data/images/2332765.jpg  \n",
            "  inflating: gqa_mini_data/images/2412822.jpg  \n",
            "  inflating: gqa_mini_data/images/2320375.jpg  \n",
            "  inflating: gqa_mini_data/images/2364279.jpg  \n",
            "  inflating: gqa_mini_data/images/2335977.jpg  \n",
            "  inflating: gqa_mini_data/images/2408661.jpg  \n",
            "  inflating: gqa_mini_data/images/2349300.jpg  \n",
            "  inflating: gqa_mini_data/images/2411640.jpg  \n",
            "  inflating: gqa_mini_data/images/2415426.jpg  \n",
            "  inflating: gqa_mini_data/images/2413731.jpg  \n",
            "  inflating: gqa_mini_data/images/2383150.jpg  \n",
            "  inflating: gqa_mini_data/images/1251.jpg  \n",
            "  inflating: gqa_mini_data/images/2363067.jpg  \n",
            "  inflating: gqa_mini_data/images/2394389.jpg  \n",
            "  inflating: gqa_mini_data/images/2330371.jpg  \n",
            "  inflating: gqa_mini_data/images/2401485.jpg  \n",
            "  inflating: gqa_mini_data/images/2389125.jpg  \n",
            "  inflating: gqa_mini_data/images/150465.jpg  \n",
            "  inflating: gqa_mini_data/images/2375215.jpg  \n",
            "  inflating: gqa_mini_data/images/1159565.jpg  \n",
            "  inflating: gqa_mini_data/images/2328660.jpg  \n",
            "  inflating: gqa_mini_data/images/2371715.jpg  \n",
            "  inflating: gqa_mini_data/images/1159571.jpg  \n",
            "  inflating: gqa_mini_data/images/1592150.jpg  \n",
            "  inflating: gqa_mini_data/images/2417344.jpg  \n",
            "  inflating: gqa_mini_data/images/2359502.jpg  \n",
            "  inflating: gqa_mini_data/images/2337568.jpg  \n",
            "  inflating: gqa_mini_data/images/2321646.jpg  \n",
            "  inflating: gqa_mini_data/images/2403870.jpg  \n",
            "  inflating: gqa_mini_data/images/2330978.jpg  \n",
            "  inflating: gqa_mini_data/images/2383556.jpg  \n",
            "  inflating: gqa_mini_data/images/2415800.jpg  \n",
            "  inflating: gqa_mini_data/images/2321732.jpg  \n",
            "  inflating: gqa_mini_data/images/2378874.jpg  \n",
            "  inflating: gqa_mini_data/images/2385251.jpg  \n",
            "  inflating: gqa_mini_data/images/2404500.jpg  \n",
            "  inflating: gqa_mini_data/images/2402170.jpg  \n",
            "  inflating: gqa_mini_data/images/2350479.jpg  \n",
            "  inflating: gqa_mini_data/images/2411118.jpg  \n",
            "  inflating: gqa_mini_data/images/2377788.jpg  \n",
            "  inflating: gqa_mini_data/images/2320175.jpg  \n",
            "  inflating: gqa_mini_data/images/2371110.jpg  \n",
            "  inflating: gqa_mini_data/images/2339961.jpg  \n",
            "  inflating: gqa_mini_data/images/2403656.jpg  \n",
            "  inflating: gqa_mini_data/images/2410831.jpg  \n",
            "  inflating: gqa_mini_data/images/2366256.jpg  \n",
            "  inflating: gqa_mini_data/images/2371500.jpg  \n",
            "  inflating: gqa_mini_data/images/2374444.jpg  \n",
            "  inflating: gqa_mini_data/images/2326316.jpg  \n",
            "  inflating: gqa_mini_data/images/2371528.jpg  \n",
            "  inflating: gqa_mini_data/images/2391382.jpg  \n",
            "  inflating: gqa_mini_data/images/2359288.jpg  \n",
            "  inflating: gqa_mini_data/images/2379158.jpg  \n",
            "  inflating: gqa_mini_data/images/2332603.jpg  \n",
            "  inflating: gqa_mini_data/images/2415781.jpg  \n",
            "  inflating: gqa_mini_data/images/2346960.jpg  \n",
            "  inflating: gqa_mini_data/images/2403083.jpg  \n",
            "  inflating: gqa_mini_data/images/2413290.jpg  \n",
            "  inflating: gqa_mini_data/images/2396980.jpg  \n",
            "  inflating: gqa_mini_data/images/2406110.jpg  \n",
            "  inflating: gqa_mini_data/images/2370752.jpg  \n",
            "  inflating: gqa_mini_data/images/2368684.jpg  \n",
            "  inflating: gqa_mini_data/images/2317904.jpg  \n",
            "  inflating: gqa_mini_data/images/2334663.jpg  \n",
            "  inflating: gqa_mini_data/images/713932.jpg  \n",
            "  inflating: gqa_mini_data/images/2322217.jpg  \n",
            "  inflating: gqa_mini_data/images/2396956.jpg  \n",
            "  inflating: gqa_mini_data/images/2316830.jpg  \n",
            "  inflating: gqa_mini_data/images/2407435.jpg  \n",
            "  inflating: gqa_mini_data/images/4065.jpg  \n",
            "  inflating: gqa_mini_data/images/2366253.jpg  \n",
            "  inflating: gqa_mini_data/images/2393543.jpg  \n",
            "  inflating: gqa_mini_data/images/2359538.jpg  \n",
            "  inflating: gqa_mini_data/images/2164.jpg  \n",
            "  inflating: gqa_mini_data/images/1451.jpg  \n",
            "  inflating: gqa_mini_data/images/2354975.jpg  \n",
            "  inflating: gqa_mini_data/images/2363117.jpg  \n",
            "  inflating: gqa_mini_data/images/1309.jpg  \n",
            "  inflating: gqa_mini_data/images/2166.jpg  \n",
            "  inflating: gqa_mini_data/images/2340094.jpg  \n",
            "  inflating: gqa_mini_data/images/2343576.jpg  \n",
            "  inflating: gqa_mini_data/images/2397041.jpg  \n",
            "  inflating: gqa_mini_data/images/2325432.jpg  \n",
            "  inflating: gqa_mini_data/images/2338463.jpg  \n",
            "  inflating: gqa_mini_data/images/1591893.jpg  \n",
            "  inflating: gqa_mini_data/images/2389069.jpg  \n",
            "  inflating: gqa_mini_data/images/2394004.jpg  \n",
            "  inflating: gqa_mini_data/images/2381227.jpg  \n",
            "  inflating: gqa_mini_data/images/2399454.jpg  \n",
            "  inflating: gqa_mini_data/images/2354803.jpg  \n",
            "  inflating: gqa_mini_data/images/1592785.jpg  \n",
            "  inflating: gqa_mini_data/images/2361884.jpg  \n",
            "  inflating: gqa_mini_data/images/2346585.jpg  \n",
            "  inflating: gqa_mini_data/images/2364081.jpg  \n",
            "  inflating: gqa_mini_data/images/2350456.jpg  \n",
            "  inflating: gqa_mini_data/images/2361853.jpg  \n",
            "  inflating: gqa_mini_data/images/2407027.jpg  \n",
            "  inflating: gqa_mini_data/images/2403241.jpg  \n",
            "  inflating: gqa_mini_data/images/2355249.jpg  \n",
            "  inflating: gqa_mini_data/images/2349884.jpg  \n",
            "  inflating: gqa_mini_data/images/2373932.jpg  \n",
            "  inflating: gqa_mini_data/images/2399469.jpg  \n",
            "  inflating: gqa_mini_data/images/286049.jpg  \n",
            "  inflating: gqa_mini_data/images/2368335.jpg  \n",
            "  inflating: gqa_mini_data/images/2398603.jpg  \n",
            "  inflating: gqa_mini_data/images/2396809.jpg  \n",
            "  inflating: gqa_mini_data/images/2367614.jpg  \n",
            "  inflating: gqa_mini_data/images/2399051.jpg  \n",
            "  inflating: gqa_mini_data/images/2355473.jpg  \n",
            "  inflating: gqa_mini_data/images/2376533.jpg  \n",
            "  inflating: gqa_mini_data/images/1592828.jpg  \n",
            "  inflating: gqa_mini_data/images/2416913.jpg  \n",
            "  inflating: gqa_mini_data/images/2407188.jpg  \n",
            "  inflating: gqa_mini_data/images/2363655.jpg  \n",
            "  inflating: gqa_mini_data/images/2396353.jpg  \n",
            "  inflating: gqa_mini_data/images/2322190.jpg  \n",
            "  inflating: gqa_mini_data/images/2412977.jpg  \n",
            "  inflating: gqa_mini_data/images/2405589.jpg  \n",
            "  inflating: gqa_mini_data/images/150294.jpg  \n",
            "  inflating: gqa_mini_data/images/2374711.jpg  \n",
            "  inflating: gqa_mini_data/images/2406862.jpg  \n",
            "  inflating: gqa_mini_data/images/2386593.jpg  \n",
            "  inflating: gqa_mini_data/images/2359867.jpg  \n",
            "  inflating: gqa_mini_data/images/2408640.jpg  \n",
            "  inflating: gqa_mini_data/images/2407956.jpg  \n",
            "  inflating: gqa_mini_data/images/2318386.jpg  \n",
            "  inflating: gqa_mini_data/images/2360956.jpg  \n",
            "  inflating: gqa_mini_data/images/2339823.jpg  \n",
            "  inflating: gqa_mini_data/images/2395263.jpg  \n",
            "  inflating: gqa_mini_data/images/2404899.jpg  \n",
            "  inflating: gqa_mini_data/images/2409173.jpg  \n",
            "  inflating: gqa_mini_data/images/2382441.jpg  \n",
            "  inflating: gqa_mini_data/images/2394237.jpg  \n",
            "  inflating: gqa_mini_data/images/2390904.jpg  \n",
            "  inflating: gqa_mini_data/images/2407837.jpg  \n",
            "  inflating: gqa_mini_data/images/2372773.jpg  \n",
            "  inflating: gqa_mini_data/images/2359521.jpg  \n",
            "  inflating: gqa_mini_data/images/2394553.jpg  \n",
            "  inflating: gqa_mini_data/images/2384768.jpg  \n",
            "  inflating: gqa_mini_data/images/2352845.jpg  \n",
            "  inflating: gqa_mini_data/images/2326849.jpg  \n",
            "  inflating: gqa_mini_data/images/2405561.jpg  \n",
            "  inflating: gqa_mini_data/images/2369838.jpg  \n",
            "  inflating: gqa_mini_data/images/2399275.jpg  \n",
            "  inflating: gqa_mini_data/images/2380687.jpg  \n",
            "  inflating: gqa_mini_data/images/2337617.jpg  \n",
            "  inflating: gqa_mini_data/images/2373914.jpg  \n",
            "  inflating: gqa_mini_data/images/2346763.jpg  \n",
            "  inflating: gqa_mini_data/images/2389884.jpg  \n",
            "  inflating: gqa_mini_data/images/2403272.jpg  \n",
            "  inflating: gqa_mini_data/images/2386750.jpg  \n",
            "  inflating: gqa_mini_data/images/2349337.jpg  \n",
            "  inflating: gqa_mini_data/images/2373726.jpg  \n",
            "  inflating: gqa_mini_data/images/2331529.jpg  \n",
            "  inflating: gqa_mini_data/images/2388392.jpg  \n",
            "  inflating: gqa_mini_data/images/2338684.jpg  \n",
            "  inflating: gqa_mini_data/images/2352307.jpg  \n",
            "  inflating: gqa_mini_data/images/2829.jpg  \n",
            "  inflating: gqa_mini_data/images/2341409.jpg  \n",
            "  inflating: gqa_mini_data/images/712995.jpg  \n",
            "  inflating: gqa_mini_data/images/1592823.jpg  \n",
            "  inflating: gqa_mini_data/images/2395660.jpg  \n",
            "  inflating: gqa_mini_data/images/640.jpg  \n",
            "  inflating: gqa_mini_data/images/2394224.jpg  \n",
            "  inflating: gqa_mini_data/images/2319666.jpg  \n",
            "  inflating: gqa_mini_data/images/2411075.jpg  \n",
            "  inflating: gqa_mini_data/images/2343795.jpg  \n",
            "  inflating: gqa_mini_data/images/2364464.jpg  \n",
            "  inflating: gqa_mini_data/images/2392494.jpg  \n",
            "  inflating: gqa_mini_data/images/2398620.jpg  \n",
            "  inflating: gqa_mini_data/images/2357723.jpg  \n",
            "  inflating: gqa_mini_data/images/2355108.jpg  \n",
            "  inflating: gqa_mini_data/images/2377222.jpg  \n",
            "  inflating: gqa_mini_data/images/2318183.jpg  \n",
            "  inflating: gqa_mini_data/images/2359134.jpg  \n",
            "  inflating: gqa_mini_data/images/2326247.jpg  \n",
            "  inflating: gqa_mini_data/images/2332540.jpg  \n",
            "  inflating: gqa_mini_data/images/2328295.jpg  \n",
            "  inflating: gqa_mini_data/images/2345722.jpg  \n",
            "  inflating: gqa_mini_data/images/2409968.jpg  \n",
            "  inflating: gqa_mini_data/images/2404733.jpg  \n",
            "  inflating: gqa_mini_data/images/2339164.jpg  \n",
            "  inflating: gqa_mini_data/images/2345285.jpg  \n",
            "  inflating: gqa_mini_data/images/2319471.jpg  \n",
            "  inflating: gqa_mini_data/images/2413461.jpg  \n",
            "  inflating: gqa_mini_data/images/2397512.jpg  \n",
            "  inflating: gqa_mini_data/images/2408096.jpg  \n",
            "  inflating: gqa_mini_data/images/2406905.jpg  \n",
            "  inflating: gqa_mini_data/images/2390719.jpg  \n",
            "  inflating: gqa_mini_data/images/1592836.jpg  \n",
            "  inflating: gqa_mini_data/images/2392867.jpg  \n",
            "  inflating: gqa_mini_data/images/1593257.jpg  \n",
            "  inflating: gqa_mini_data/images/2403314.jpg  \n",
            "  inflating: gqa_mini_data/images/1664.jpg  \n",
            "  inflating: gqa_mini_data/images/2349293.jpg  \n",
            "  inflating: gqa_mini_data/images/4046.jpg  \n",
            "  inflating: gqa_mini_data/images/2363485.jpg  \n",
            "  inflating: gqa_mini_data/images/2398623.jpg  \n",
            "  inflating: gqa_mini_data/images/2330787.jpg  \n",
            "  inflating: gqa_mini_data/images/2316998.jpg  \n",
            "  inflating: gqa_mini_data/images/2380118.jpg  \n",
            "  inflating: gqa_mini_data/images/2346598.jpg  \n",
            "  inflating: gqa_mini_data/images/713246.jpg  \n",
            "  inflating: gqa_mini_data/images/2322778.jpg  \n",
            "  inflating: gqa_mini_data/images/2368117.jpg  \n",
            "  inflating: gqa_mini_data/images/2410552.jpg  \n",
            "  inflating: gqa_mini_data/images/2315671.jpg  \n",
            "  inflating: gqa_mini_data/images/2394434.jpg  \n",
            "  inflating: gqa_mini_data/images/2370995.jpg  \n",
            "  inflating: gqa_mini_data/images/2395528.jpg  \n",
            "  inflating: gqa_mini_data/images/2382040.jpg  \n",
            "  inflating: gqa_mini_data/images/2414537.jpg  \n",
            "  inflating: gqa_mini_data/images/2352665.jpg  \n",
            "  inflating: gqa_mini_data/images/2338494.jpg  \n",
            "  inflating: gqa_mini_data/images/2398636.jpg  \n",
            "  inflating: gqa_mini_data/images/2350099.jpg  \n",
            "  inflating: gqa_mini_data/images/2361250.jpg  \n",
            "  inflating: gqa_mini_data/images/871.jpg  \n",
            "  inflating: gqa_mini_data/images/2411951.jpg  \n",
            "  inflating: gqa_mini_data/images/2390033.jpg  \n",
            "  inflating: gqa_mini_data/images/2379691.jpg  \n",
            "  inflating: gqa_mini_data/images/2344510.jpg  \n",
            "  inflating: gqa_mini_data/images/2388898.jpg  \n",
            "  inflating: gqa_mini_data/images/2412525.jpg  \n",
            "  inflating: gqa_mini_data/images/2398321.jpg  \n",
            "  inflating: gqa_mini_data/images/2329529.jpg  \n",
            "  inflating: gqa_mini_data/images/2412096.jpg  \n",
            "  inflating: gqa_mini_data/images/2416582.jpg  \n",
            "  inflating: gqa_mini_data/images/2407302.jpg  \n",
            "  inflating: gqa_mini_data/images/2407316.jpg  \n",
            "  inflating: gqa_mini_data/images/2381472.jpg  \n",
            "  inflating: gqa_mini_data/images/2377284.jpg  \n",
            "  inflating: gqa_mini_data/images/2347810.jpg  \n",
            "  inflating: gqa_mini_data/images/2353681.jpg  \n",
            "  inflating: gqa_mini_data/images/2392036.jpg  \n",
            "  inflating: gqa_mini_data/images/2363781.jpg  \n",
            "  inflating: gqa_mini_data/images/2410535.jpg  \n",
            "  inflating: gqa_mini_data/images/2377939.jpg  \n",
            "  inflating: gqa_mini_data/images/2322938.jpg  \n",
            "  inflating: gqa_mini_data/images/2349141.jpg  \n",
            "  inflating: gqa_mini_data/images/2336795.jpg  \n",
            "  inflating: gqa_mini_data/images/2411617.jpg  \n",
            "  inflating: gqa_mini_data/images/2410938.jpg  \n",
            "  inflating: gqa_mini_data/images/2401766.jpg  \n",
            "  inflating: gqa_mini_data/images/2394917.jpg  \n",
            "  inflating: gqa_mini_data/images/2401558.jpg  \n",
            "  inflating: gqa_mini_data/images/2333969.jpg  \n",
            "  inflating: gqa_mini_data/images/2413606.jpg  \n",
            "  inflating: gqa_mini_data/images/2326380.jpg  \n",
            "  inflating: gqa_mini_data/images/2384045.jpg  \n",
            "  inflating: gqa_mini_data/images/2416998.jpg  \n",
            "  inflating: gqa_mini_data/images/2365293.jpg  \n",
            "  inflating: gqa_mini_data/images/2373618.jpg  \n",
            "  inflating: gqa_mini_data/images/2401943.jpg  \n",
            "  inflating: gqa_mini_data/images/2401016.jpg  \n",
            "  inflating: gqa_mini_data/images/2412693.jpg  \n",
            "  inflating: gqa_mini_data/images/150431.jpg  \n",
            "  inflating: gqa_mini_data/images/2331577.jpg  \n",
            "  inflating: gqa_mini_data/images/2403013.jpg  \n",
            "  inflating: gqa_mini_data/images/2357631.jpg  \n",
            "  inflating: gqa_mini_data/images/2322084.jpg  \n",
            "  inflating: gqa_mini_data/images/2377085.jpg  \n",
            "  inflating: gqa_mini_data/images/2351043.jpg  \n",
            "  inflating: gqa_mini_data/images/2385560.jpg  \n",
            "  inflating: gqa_mini_data/images/2394445.jpg  \n",
            "  inflating: gqa_mini_data/images/2381909.jpg  \n",
            "  inflating: gqa_mini_data/images/2402873.jpg  \n",
            "  inflating: gqa_mini_data/images/2360922.jpg  \n",
            "  inflating: gqa_mini_data/images/2350177.jpg  \n",
            "  inflating: gqa_mini_data/images/2368417.jpg  \n",
            "  inflating: gqa_mini_data/images/2392593.jpg  \n",
            "  inflating: gqa_mini_data/images/2322495.jpg  \n",
            "  inflating: gqa_mini_data/images/2389951.jpg  \n",
            "  inflating: gqa_mini_data/images/2400889.jpg  \n",
            "  inflating: gqa_mini_data/images/2385414.jpg  \n",
            "  inflating: gqa_mini_data/images/2326340.jpg  \n",
            "  inflating: gqa_mini_data/images/2321349.jpg  \n",
            "  inflating: gqa_mini_data/images/2374374.jpg  \n",
            "  inflating: gqa_mini_data/images/2317199.jpg  \n",
            "  inflating: gqa_mini_data/images/2364188.jpg  \n",
            "  inflating: gqa_mini_data/images/4581.jpg  \n",
            "  inflating: gqa_mini_data/images/2380540.jpg  \n",
            "  inflating: gqa_mini_data/images/2410122.jpg  \n",
            "  inflating: gqa_mini_data/images/2370499.jpg  \n",
            "  inflating: gqa_mini_data/images/2329329.jpg  \n",
            "  inflating: gqa_mini_data/images/2378172.jpg  \n",
            "  inflating: gqa_mini_data/images/2328745.jpg  \n",
            "  inflating: gqa_mini_data/images/2321991.jpg  \n",
            "  inflating: gqa_mini_data/images/2353334.jpg  \n",
            "  inflating: gqa_mini_data/images/2389606.jpg  \n",
            "  inflating: gqa_mini_data/images/2407099.jpg  \n",
            "  inflating: gqa_mini_data/images/713232.jpg  \n",
            "  inflating: gqa_mini_data/images/2374204.jpg  \n",
            "  inflating: gqa_mini_data/images/2388901.jpg  \n",
            "  inflating: gqa_mini_data/images/2363786.jpg  \n",
            "  inflating: gqa_mini_data/images/2318334.jpg  \n",
            "  inflating: gqa_mini_data/images/2409923.jpg  \n",
            "  inflating: gqa_mini_data/images/2373556.jpg  \n",
            "  inflating: gqa_mini_data/images/2400278.jpg  \n",
            "  inflating: gqa_mini_data/images/2321006.jpg  \n",
            "  inflating: gqa_mini_data/images/2411201.jpg  \n",
            "  inflating: gqa_mini_data/images/2371962.jpg  \n",
            "  inflating: gqa_mini_data/images/2373191.jpg  \n",
            "  inflating: gqa_mini_data/images/2394078.jpg  \n",
            "  inflating: gqa_mini_data/images/2358300.jpg  \n",
            "  inflating: gqa_mini_data/images/2382829.jpg  \n",
            "  inflating: gqa_mini_data/images/2375251.jpg  \n",
            "  inflating: gqa_mini_data/images/2381139.jpg  \n",
            "  inflating: gqa_mini_data/images/61515.jpg  \n",
            "  inflating: gqa_mini_data/images/2369452.jpg  \n",
            "  inflating: gqa_mini_data/images/2378205.jpg  \n",
            "  inflating: gqa_mini_data/images/497941.jpg  \n",
            "  inflating: gqa_mini_data/images/2333760.jpg  \n",
            "  inflating: gqa_mini_data/images/2398495.jpg  \n",
            "  inflating: gqa_mini_data/images/2371545.jpg  \n",
            "  inflating: gqa_mini_data/images/2343285.jpg  \n",
            "  inflating: gqa_mini_data/images/2402727.jpg  \n",
            "  inflating: gqa_mini_data/images/2373391.jpg  \n",
            "  inflating: gqa_mini_data/images/152.jpg  \n",
            "  inflating: gqa_mini_data/images/2368955.jpg  \n",
            "  inflating: gqa_mini_data/images/2321616.jpg  \n",
            "  inflating: gqa_mini_data/images/2333548.jpg  \n",
            "  inflating: gqa_mini_data/images/2388453.jpg  \n",
            "  inflating: gqa_mini_data/images/2341914.jpg  \n",
            "  inflating: gqa_mini_data/images/2366415.jpg  \n",
            "  inflating: gqa_mini_data/images/4623.jpg  \n",
            "  inflating: gqa_mini_data/images/2361144.jpg  \n",
            "  inflating: gqa_mini_data/images/2363035.jpg  \n",
            "  inflating: gqa_mini_data/images/2345964.jpg  \n",
            "  inflating: gqa_mini_data/images/1159669.jpg  \n",
            "  inflating: gqa_mini_data/images/2394495.jpg  \n",
            "  inflating: gqa_mini_data/images/2410531.jpg  \n",
            "  inflating: gqa_mini_data/images/2390630.jpg  \n",
            "  inflating: gqa_mini_data/images/2411174.jpg  \n",
            "  inflating: gqa_mini_data/images/2380350.jpg  \n",
            "  inflating: gqa_mini_data/images/2345742.jpg  \n",
            "  inflating: gqa_mini_data/images/2402861.jpg  \n",
            "  inflating: gqa_mini_data/images/2379736.jpg  \n",
            "  inflating: gqa_mini_data/images/2369519.jpg  \n",
            "  inflating: gqa_mini_data/images/2387629.jpg  \n",
            "  inflating: gqa_mini_data/images/2318647.jpg  \n",
            "  inflating: gqa_mini_data/images/2370262.jpg  \n",
            "  inflating: gqa_mini_data/images/2330134.jpg  \n",
            "  inflating: gqa_mini_data/images/2329303.jpg  \n",
            "  inflating: gqa_mini_data/images/2404784.jpg  \n",
            "  inflating: gqa_mini_data/images/2324784.jpg  \n",
            "  inflating: gqa_mini_data/images/2359035.jpg  \n",
            "  inflating: gqa_mini_data/images/2359753.jpg  \n",
            "  inflating: gqa_mini_data/images/2350039.jpg  \n",
            "  inflating: gqa_mini_data/images/2414434.jpg  \n",
            "  inflating: gqa_mini_data/images/2343045.jpg  \n",
            "  inflating: gqa_mini_data/images/2383513.jpg  \n",
            "  inflating: gqa_mini_data/images/2367681.jpg  \n",
            "  inflating: gqa_mini_data/images/2318055.jpg  \n",
            "  inflating: gqa_mini_data/images/2379440.jpg  \n",
            "  inflating: gqa_mini_data/images/2355419.jpg  \n",
            "  inflating: gqa_mini_data/images/2342171.jpg  \n",
            "  inflating: gqa_mini_data/images/2367483.jpg  \n",
            "  inflating: gqa_mini_data/images/2324545.jpg  \n",
            "  inflating: gqa_mini_data/images/2377109.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Vieira"
      ],
      "metadata": {
        "id": "CDSI9xix3XT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeuGvxgmS3Ht",
        "outputId": "faedfd9f-6f74-484f-d985-ac7ac78003c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-07 03:36:59--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/b5c94623-4ec7-46ab-b2c7-52c18f0951f6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=f66db6d1e55e388b8e5e3f360a705ae9cc5d4432da70093ab7613a0dbbd9f4c2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:36:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/b5c94623-4ec7-46ab-b2c7-52c18f0951f6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=f66db6d1e55e388b8e5e3f360a705ae9cc5d4432da70093ab7613a0dbbd9f4c2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6976132 (6.7M) [application/octet-stream]\n",
            "Saving to: ‘vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl’\n",
            "\n",
            "vieira-0.2.2-cp310- 100%[===================>]   6.65M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-07 03:36:59 (59.6 MB/s) - ‘vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl’ saved [6976132/6976132]\n",
            "\n",
            "--2023-11-07 03:36:59--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_ext-0.2.2-py3-none-any.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/bb388597-9850-4515-8f6f-a6d37828a912?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=4f7913f9cb2ffbde60cbc08ecbc73885b31204f803fc01d1f6fc5af3e5b049d7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_ext-0.2.2-py3-none-any.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:36:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/bb388597-9850-4515-8f6f-a6d37828a912?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=4f7913f9cb2ffbde60cbc08ecbc73885b31204f803fc01d1f6fc5af3e5b049d7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_ext-0.2.2-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5872 (5.7K) [application/octet-stream]\n",
            "Saving to: ‘vieira_ext-0.2.2-py3-none-any.whl’\n",
            "\n",
            "vieira_ext-0.2.2-py 100%[===================>]   5.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-07 03:36:59 (65.4 MB/s) - ‘vieira_ext-0.2.2-py3-none-any.whl’ saved [5872/5872]\n",
            "\n",
            "--2023-11-07 03:36:59--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_gpu-0.0.1-py3-none-any.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/f973b9bc-e60d-452a-b253-15810a413386?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=38c921cacbdffaf755f621c295dba99b47d3c13b77abcdc88cc9fc36a3787d13&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_gpu-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:37:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/f973b9bc-e60d-452a-b253-15810a413386?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033659Z&X-Amz-Expires=300&X-Amz-Signature=38c921cacbdffaf755f621c295dba99b47d3c13b77abcdc88cc9fc36a3787d13&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_gpu-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1973 (1.9K) [application/octet-stream]\n",
            "Saving to: ‘vieira_gpu-0.0.1-py3-none-any.whl’\n",
            "\n",
            "vieira_gpu-0.0.1-py 100%[===================>]   1.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-07 03:37:00 (39.2 MB/s) - ‘vieira_gpu-0.0.1-py3-none-any.whl’ saved [1973/1973]\n",
            "\n",
            "--2023-11-07 03:37:00--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_gpt-0.0.1-py3-none-any.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/c5971eae-7c2a-4629-b01f-c9b8cabed7cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=e2599403558eca8d834ebad122a18877566998b29e7d19fbe4a931122d8c49ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_gpt-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:37:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/c5971eae-7c2a-4629-b01f-c9b8cabed7cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=e2599403558eca8d834ebad122a18877566998b29e7d19fbe4a931122d8c49ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_gpt-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9677 (9.5K) [application/octet-stream]\n",
            "Saving to: ‘vieira_gpt-0.0.1-py3-none-any.whl’\n",
            "\n",
            "vieira_gpt-0.0.1-py 100%[===================>]   9.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-07 03:37:00 (92.3 MB/s) - ‘vieira_gpt-0.0.1-py3-none-any.whl’ saved [9677/9677]\n",
            "\n",
            "--2023-11-07 03:37:00--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_transformers-0.0.1-py3-none-any.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/7227bd61-f140-4514-b640-9164a6acae04?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=ef7c95f1c0062e00c4a4e0efdda15f1c9b99ba77ee34ba8a9d1ac1270e745778&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_transformers-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:37:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/7227bd61-f140-4514-b640-9164a6acae04?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=ef7c95f1c0062e00c4a4e0efdda15f1c9b99ba77ee34ba8a9d1ac1270e745778&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_transformers-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7091 (6.9K) [application/octet-stream]\n",
            "Saving to: ‘vieira_transformers-0.0.1-py3-none-any.whl’\n",
            "\n",
            "vieira_transformers 100%[===================>]   6.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-07 03:37:00 (73.6 MB/s) - ‘vieira_transformers-0.0.1-py3-none-any.whl’ saved [7091/7091]\n",
            "\n",
            "--2023-11-07 03:37:00--  https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_opencv-0.0.1-py3-none-any.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/0a859229-42b6-471f-bbff-70120c7e1dc2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=fbe033f86ad4e48e77b6820e17f70d062230ce24301ee909b5df7f8d5a474cfc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_opencv-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-07 03:37:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/713596004/0a859229-42b6-471f-bbff-70120c7e1dc2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T033700Z&X-Amz-Expires=300&X-Amz-Signature=fbe033f86ad4e48e77b6820e17f70d062230ce24301ee909b5df7f8d5a474cfc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=713596004&response-content-disposition=attachment%3B%20filename%3Dvieira_opencv-0.0.1-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5833 (5.7K) [application/octet-stream]\n",
            "Saving to: ‘vieira_opencv-0.0.1-py3-none-any.whl’\n",
            "\n",
            "vieira_opencv-0.0.1 100%[===================>]   5.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-07 03:37:00 (56.0 MB/s) - ‘vieira_opencv-0.0.1-py3-none-any.whl’ saved [5833/5833]\n",
            "\n",
            "Processing ./vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl\n",
            "Installing collected packages: vieira\n",
            "Successfully installed vieira-0.2.2\n",
            "Processing ./vieira_ext-0.2.2-py3-none-any.whl\n",
            "Installing collected packages: vieira-ext\n",
            "Successfully installed vieira-ext-0.2.2\n",
            "Processing ./vieira_gpu-0.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vieira-gpu==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpu==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vieira-gpu==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vieira-gpu==0.0.1) (1.3.0)\n",
            "Installing collected packages: vieira-gpu\n",
            "Successfully installed vieira-gpu-0.0.1\n",
            "Processing ./vieira_gpt-0.0.1-py3-none-any.whl\n",
            "Collecting openai~=0.28 (from vieira-gpt==0.0.1)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vieira-gpt==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai~=0.28->vieira-gpt==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai~=0.28->vieira-gpt==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai~=0.28->vieira-gpt==0.0.1) (3.8.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-gpt==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28->vieira-gpt==0.0.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28->vieira-gpt==0.0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28->vieira-gpt==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai~=0.28->vieira-gpt==0.0.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai~=0.28->vieira-gpt==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vieira-gpt==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vieira-gpt==0.0.1) (1.3.0)\n",
            "Installing collected packages: openai, vieira-gpt\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1 vieira-gpt-0.0.1\n",
            "Processing ./vieira_transformers-0.0.1-py3-none-any.whl\n",
            "Collecting transformers (from vieira-transformers==0.0.1)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from vieira-transformers==0.0.1) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vieira-transformers==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python->vieira-transformers==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-transformers==0.0.1) (2.1.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->vieira-transformers==0.0.1)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->vieira-transformers==0.0.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->vieira-transformers==0.0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->vieira-transformers==0.0.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->vieira-transformers==0.0.1) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers->vieira-transformers==0.0.1)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->vieira-transformers==0.0.1)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->vieira-transformers==0.0.1) (4.66.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->vieira-transformers==0.0.1)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vieira-transformers==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->vieira-transformers==0.0.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->vieira-transformers==0.0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->vieira-transformers==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->vieira-transformers==0.0.1) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vieira-transformers==0.0.1) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, vieira-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0 vieira-transformers-0.0.1\n",
            "Processing ./vieira_opencv-0.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from vieira-opencv==0.0.1) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vieira-opencv==0.0.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python->vieira-opencv==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vieira-opencv==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vieira-opencv==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vieira-opencv==0.0.1) (1.3.0)\n",
            "Installing collected packages: vieira-opencv\n",
            "Successfully installed vieira-opencv-0.0.1\n"
          ]
        }
      ],
      "source": [
        "# Download and install Vieira\n",
        "# The default python version is for 3.10, you may change the link according to your python versions.\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_ext-0.2.2-py3-none-any.whl\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_gpu-0.0.1-py3-none-any.whl\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_gpt-0.0.1-py3-none-any.whl\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_transformers-0.0.1-py3-none-any.whl\n",
        "!wget https://github.com/vieira-artifact/vieira-artifact-aaai24/releases/download/v0.2.2/vieira_opencv-0.0.1-py3-none-any.whl\n",
        "!pip install vieira-0.2.2-cp310-cp310-manylinux_2_31_x86_64.whl\n",
        "!pip install vieira_ext-0.2.2-py3-none-any.whl\n",
        "!pip install vieira_gpu-0.0.1-py3-none-any.whl\n",
        "!pip install vieira_gpt-0.0.1-py3-none-any.whl\n",
        "!pip install vieira_transformers-0.0.1-py3-none-any.whl\n",
        "!pip install vieira_opencv-0.0.1-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Vieira!"
      ],
      "metadata": {
        "id": "KQIsS1aR3bo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import vieira and related plugins\n",
        "import vieira\n",
        "import vieira_ext"
      ],
      "metadata": {
        "id": "XUCT02_IeoCj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Vieira plugins.\n",
        "\n",
        "In this application, GPU, GPT, Transformers, and OpenCV plugins will be enabled."
      ],
      "metadata": {
        "id": "LT6yfd9afiLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your OpenAI API key if you want to run the example\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =  \"YOUR-OPENAI-API-KEY-HERE\""
      ],
      "metadata": {
        "id": "UidSskjitIqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Vieira plugins\n",
        "import argparse\n",
        "plugins = vieira_ext.PluginRegistry()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "plugins.setup_argument_parser(parser)\n",
        "known_args, unknown_args = parser.parse_known_args()\n",
        "plugins.configure(known_args, unknown_args)"
      ],
      "metadata": {
        "id": "CSsFOp4kflnl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code loading the dataset and GPT-4 generated programs."
      ],
      "metadata": {
        "id": "rm667iKb3rU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset\n",
        "import pickle\n",
        "import io\n",
        "import tokenize\n",
        "\n",
        "GQA_PATH = '/content/gqa_mini_data/'\n",
        "\n",
        "# Used to parse each data point's input program\n",
        "class Program:\n",
        "  def __init__(self, prog_str, init_state=None):\n",
        "    self.prog_str = prog_str\n",
        "    self.state = init_state if init_state is not None else dict()\n",
        "    self.instructions = self.prog_str.split('\\n')\n",
        "    self.progs = [parse_step(i) for i in self.instructions]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.prog_str\n",
        "\n",
        "def parse_step(step_str, partial=False):\n",
        "  tokens = list(tokenize.generate_tokens(io.StringIO(step_str).readline))\n",
        "  output_var = tokens[0].string\n",
        "  step_name = tokens[2].string\n",
        "  parsed_result = dict(\n",
        "    output_var=output_var,\n",
        "    step_name=step_name)\n",
        "  if partial:\n",
        "    return parsed_result\n",
        "\n",
        "  arg_tokens = [token for token in tokens[4:-3] if token.string not in [',','=']]\n",
        "  num_tokens = len(arg_tokens) // 2\n",
        "  args = dict()\n",
        "  for i in range(num_tokens):\n",
        "    args[arg_tokens[2*i].string] = arg_tokens[2*i+1].string\n",
        "  parsed_result['args'] = args\n",
        "  return parsed_result\n",
        "\n",
        "def get_dataset():\n",
        "  with open(GQA_PATH + 'mini_question_no_crop.pkl', 'rb') as f:\n",
        "    return pickle.load(f)"
      ],
      "metadata": {
        "id": "CQjHMdkb87Zc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Vieira Program"
      ],
      "metadata": {
        "id": "zizrca5V3wOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vieira context with Vieira code\n",
        "def create_context():\n",
        "  ctx = vieira.Context(provenance=\"topkproofs\")\n",
        "  plugins.load_into_ctx(ctx)\n",
        "  ctx.set_iter_limit(100)\n",
        "\n",
        "  ctx.add_program(\"\"\"\n",
        "type question(q: String)\n",
        "\n",
        "@gpt_extract_info(\n",
        "  header=\\\"\\\"\\\"\n",
        "Please come up with a sequence of evaluation steps that can turn a natural language question into a semantic program.\n",
        "Each step looks like \"VAR = EXPR\", where EXPR calls a function like the following:\n",
        "\n",
        "- LOC(image, object), which can locate objects on the image, returning bounding boxes\n",
        "- CROP(image, box), CROP_LEFTOF, CROP_ABOVE, CROP_BELOW, CROP_RIGHTOF, which can crop the image according to the bounding box\n",
        "- VQA(image, str), which performs simple visual question answering on the image given the question string\n",
        "- COUNT(box), which can count the number of bounding boxes\n",
        "- EVAL(str), where the str is a Python expression that can be evaluated.\n",
        "\n",
        "Note that the last statement should assign a variable named `FINAL_RESULT`.\n",
        "\\\"\\\"\\\",\n",
        "  prompts=[\"What are the evaluation steps?\"],\n",
        "  examples=[\n",
        "    (\n",
        "      [\"Are there bags to the right of the white vehicle?\"],\n",
        "      [[\n",
        "        (1, \"BOX0\", \"LOC(IMAGE,'white vehicle')\"),\n",
        "        (2, \"IMAGE0\", \"CROP_RIGHTOF(IMAGE,BOX0)\"),\n",
        "        (3, \"BOX1\", \"LOC(IMAGE0,'bags')\"),\n",
        "        (4, \"ANSWER0\", \"COUNT(BOX1)\"),\n",
        "        (5, \"FINAL_RESULT\", \"EVAL(\\\\\"'yes' if {ANSWER0} > 0 else 'no'\\\\\")\"),\n",
        "      ]]\n",
        "    ),\n",
        "    (\n",
        "      [\"Is the person that is to the right of the hydrant wearing a jacket?\"],\n",
        "      [[\n",
        "        (1, \"BOX0\", \"LOC(IMAGE,'hydrant')\"),\n",
        "        (2, \"IMAGE0\", \"CROP_RIGHTOF(IMAGE,BOX0)\"),\n",
        "        (3, \"BOX1\", \"LOC(IMAGE0,'person')\"),\n",
        "        (4, \"IMAGE1\", \"CROP(IMAGE0,BOX1)\"),\n",
        "        (5, \"ANSWER0\", \"VQA(IMAGE1,'Is the person wearing a jacket?')\"),\n",
        "        (6, \"FINAL_RESULT\", \"RESULT(ANSWER0)\"),\n",
        "      ]]\n",
        "    )\n",
        "  ],\n",
        "  model=\"gpt-4\",\n",
        "  debug=true,\n",
        ")\n",
        "type parse_steps(bound question: String, step: u32, var: String, expr: Entity)\n",
        "\n",
        "rel step(step, var, expr) = question(q) and parse_steps(q, step, var, expr)\n",
        "\n",
        "@owl_vit(output_fields=[\"bbox-x\", \"bbox-y\", \"bbox-w\", \"bbox-h\"], limit=10)\n",
        "type find_object(bound img: Tensor, bound object: String, id: u32, x: u32, y: u32, w: u32, h: u32)\n",
        "\n",
        "@vilt(top=1)\n",
        "type vqa(bound img: Tensor, bound question: String, answer: String)\n",
        "\n",
        "@py_eval\n",
        "type $py_eval_string(s: String) -> String\n",
        "\n",
        "type Expr = IMAGE(String)\n",
        "          | CROP(String, String)\n",
        "          | CROP_ABOVE(String, String)\n",
        "          | CROP_BELOW(String, String)\n",
        "          | CROP_LEFTOF(String, String)\n",
        "          | CROP_RIGHTOF(String, String)\n",
        "          | LOC(String, String)\n",
        "          | VQA(String, String)\n",
        "          | COUNT(String)\n",
        "          | EVAL(String)\n",
        "\n",
        "type process_eval_string(bound id: u32, bound str: String, processed: String)\n",
        "rel process_eval_string(n, s, s) = n == 0\n",
        "rel process_eval_string(n + 1, str, $string_replace(str_to_eval, $format(\"{{}}\", var), val as String)) = var_value_int(var, val, n) and process_eval_string(n, str, str_to_eval)\n",
        "rel process_eval_string(n + 1, str, $string_replace(str_to_eval, $format(\"{{}}\", var), val as String)) = var_value_string(var, val, n) and process_eval_string(n, str, str_to_eval)\n",
        "rel process_eval_string(n + 1, str, str_to_eval) = step(n, _, e) and case e is LOC(_, _) and process_eval_string(n, str, str_to_eval)\n",
        "rel process_eval_string(n + 1, str, str_to_eval) = var_value_tensor(_, _, n) and process_eval_string(n, str, str_to_eval)\n",
        "\n",
        "rel eval_image(e, $load_image(img_path)) = case e is IMAGE(img_path)\n",
        "rel eval_image(e, $crop_image(img, x, y, w, h)) = case e is CROP(img_var, box_var) and var_value_tensor(img_var, img, _) and var_value_bbox(box_var, _, x, y, w, h, _)\n",
        "rel eval_image(e, $crop_image(img, x, y, w, h, \"above\")) = case e is CROP_ABOVE(img_var, box_var) and var_value_tensor(img_var, img, _) and var_value_bbox(box_var, _, x, y, w, h, _)\n",
        "rel eval_image(e, $crop_image(img, x, y, w, h, \"below\")) = case e is CROP_BELOW(img_var, box_var) and var_value_tensor(img_var, img, _) and var_value_bbox(box_var, _, x, y, w, h, _)\n",
        "rel eval_image(e, $crop_image(img, x, y, w, h, \"left\")) = case e is CROP_LEFTOF(img_var, box_var) and var_value_tensor(img_var, img, _) and var_value_bbox(box_var, _, x, y, w, h, _)\n",
        "rel eval_image(e, $crop_image(img, x, y, w, h, \"right\")) = case e is CROP_RIGHTOF(img_var, box_var) and var_value_tensor(img_var, img, _) and var_value_bbox(box_var, _, x, y, w, h, _)\n",
        "rel eval_bbox(e, id, x, y, w, h) = case e is LOC(img_name, object) and var_value_tensor(img_name, image, _) and find_object(image, object, id, x, y, w, h)\n",
        "\n",
        "type eval_string(bound id: u32, bound e: Expr, s: String)\n",
        "rel eval_string(id, e, answer) = case e is VQA(img_name, question) and var_value_tensor(img_name, image, _) and vqa(image, question, answer)\n",
        "rel eval_string(id, e, $py_eval_string(str_to_eval)) = case e is EVAL(str) and process_eval_string(id, str, str_to_eval)\n",
        "rel eval_string(id, e, result) = case e is RESULT(var_name) and var_value_string(var_name, result, _)\n",
        "rel eval_string(id, e, result as String) = case e is RESULT(var_name) and var_value_int(var_name, result, _)\n",
        "\n",
        "type eval_int(e: Expr, n: usize)\n",
        "rel eval_int(e, cnt) = cnt := count(bid: var_value_bbox(box_name, bid, _, _, _, _, _) where e: case e is COUNT(box_name))\n",
        "\n",
        "rel var_value_tensor(var_name, image, id) = step(id, var_name, expr) and eval_image(expr, image)\n",
        "rel var_value_bbox(var_name, id, x, y, w, h, sid) = step(sid, var_name, expr) and eval_bbox(expr, id, x, y, w, h)\n",
        "rel var_value_string(var_name, str, id) = step(id, var_name, expr) and eval_string(id, expr, str)\n",
        "rel var_value_int(var_name, val, id) = step(id, var_name, expr) and eval_int(expr, val)\n",
        "\n",
        "rel final_result(answer) = var_value_string(\"FINAL_RESULT\", answer, _)\n",
        "\n",
        "query final_result\n",
        "  \"\"\")\n",
        "\n",
        "  ctx.set_non_probabilistic([\"question\", \"step\"])\n",
        "  return ctx"
      ],
      "metadata": {
        "id": "cNuQWRJ5hpkX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Scripts"
      ],
      "metadata": {
        "id": "FVVbMdu839Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Accuracy is evaluated with string matching @ best k predictions\n",
        "def check_prediction(predictions, ground_truth, k):\n",
        "  return any(ground_truth in pred or pred in ground_truth for pred in predictions[:k])\n",
        "\n",
        "# Test a single data point\n",
        "def test_one(context, id, testcase):\n",
        "  ctx = context.clone()\n",
        "\n",
        "  image_path = GQA_PATH + \"images/\" + testcase[\"imageId\"] + \".jpg\"\n",
        "  ctx.add_facts(\"step\", [(0, \"IMAGE\", f'IMAGE(\"{image_path}\")')])\n",
        "  ctx.add_facts(\"question\", [(testcase[\"question\"],)])\n",
        "  ctx.run()\n",
        "  result = list(ctx.relation(\"final_result\"))\n",
        "\n",
        "  if result:\n",
        "    result.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [tup[1][0] for tup in result[:5]]\n",
        "  else:\n",
        "    return [\"no\"]"
      ],
      "metadata": {
        "id": "Uuisl3O37M1g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Experiment!\n",
        "\n",
        "We are running the program on a specific test case to save cost."
      ],
      "metadata": {
        "id": "sUhlx2Jp3-4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment on the dataset\n",
        "context = create_context()\n",
        "\n",
        "data = get_dataset()\n",
        "id = \"00464293\"\n",
        "testcase = data[id]\n",
        "results = {}\n",
        "correct = {1: 0, 3: 0, 5: 0}\n",
        "match_substring = lambda s1, s2: s1 in s2 or s2 in s1\n",
        "total = 0\n",
        "\n",
        "print(testcase)"
      ],
      "metadata": {
        "id": "NdKNxfZZ5XHb",
        "outputId": "e4aa0f84-4d9d-430e-c87a-7b1ab3c99cf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'semantic': [{'operation': 'select', 'dependencies': [], 'argument': 'cabinet (2967489)'}, {'operation': 'filter color', 'dependencies': [0], 'argument': 'light brown'}, {'operation': 'exist', 'dependencies': [1], 'argument': '?'}, {'operation': 'select', 'dependencies': [], 'argument': 'coffee table (-) '}, {'operation': 'filter color', 'dependencies': [3], 'argument': 'light brown'}, {'operation': 'exist', 'dependencies': [4], 'argument': '?'}, {'operation': 'or', 'dependencies': [2, 5], 'argument': ''}], 'entailed': ['00464294'], 'equivalent': ['00464293'], 'question': 'Is there either a light brown cabinet or coffee table?', 'imageId': '2329329', 'isBalanced': False, 'groups': {'global': None, 'local': '04-cabinet_light brown'}, 'answer': 'yes', 'semanticStr': 'select: cabinet (2967489)->filter color: light brown [0]->exist: ? [1]->select: coffee table (-) ->filter color: light brown [3]->exist: ? [4]->or:  [2, 5]', 'annotations': {'answer': {}, 'question': {'6': '2967489'}, 'fullAnswer': {'6': '2967489'}}, 'types': {'detailed': 'existAttrOr', 'semantic': 'obj', 'structural': 'logical'}, 'fullAnswer': 'Yes, there is a light brown cabinet.', 'prog': BOX0=LOC(image=IMAGE,object='light brown cabinet')\n",
            "BOX1=LOC(image=IMAGE,object='coffee table')\n",
            "ANSWER0=COUNT(box=BOX0)\n",
            "ANSWER1=COUNT(box=BOX1)\n",
            "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
            "FINAL_RESULT=RESULT(var=ANSWER2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = testcase[\"answer\"]\n",
        "predictions = test_one(context, id, testcase)\n",
        "results[id] = predictions + [ground_truth]\n",
        "\n",
        "for k in (1, 3, 5):\n",
        "  if check_prediction(predictions, ground_truth, k):\n",
        "    correct[k] += 1\n",
        "total += 1\n",
        "\n",
        "print(f\"ground truth: {ground_truth}, predictions: {predictions}\")\n",
        "print(f\"total: {total}, correct: {correct}\")\n",
        "print(testcase[\"prog\"].prog_str)"
      ],
      "metadata": {
        "id": "02wg4WFliMU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812e72ad-b69f-48f1-ce7d-648e38ab817d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Sending request: {'role': 'user', 'content': 'question: Is there either a light brown cabinet or coffee table?\n What are the evaluation steps?'}\n"
            "> Obtained response: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"[{\\\"step\\": 1, \\\"var\\": \\\"BOX0\", \\\"expr\\\": \\\"LOC(IMAGE,'light brown cabinet')\\\"}, {\\\"step\\\": 2, \\\"var\\\": \\\"BOX1\\\", \\\"expr\\\": \\\"LOC(IMAGE,'coffee table')\\\"}, {\\\"step\\\": 3, \\\"var\\\": \\\"COUNT0\\\", \\\"expr\\\": \\\"COUNT(BOX0)\\\"}, {\\\"step\\\": 4, \\\"var\\\": \\\"COUNT1\\\", \\\"expr\\\": \\\"COUNT(BOX1)\\\"}, {\\\"step\\\": 5, \\\"var\\\": \\\"FINAL_RESULT\\\", \\\"expr\\\": \\\"EVAL(\\\\\"'yes' if {COUNT0} > 0 or {COUNT1} > 0 else 'no'\\\\\")\\\"}]\"\n",
            "}\n",
            "ground truth: yes, predictions: ['yes']\n",
            "total: 1, correct: {1: 1, 3: 1, 5: 1}\n",
            "BOX0=LOC(image=IMAGE,object='light brown cabinet')\n",
            "BOX1=LOC(image=IMAGE,object='coffee table')\n",
            "ANSWER0=COUNT(box=BOX0)\n",
            "ANSWER1=COUNT(box=BOX1)\n",
            "ANSWER2=EVAL(expr=\"'yes' if {ANSWER0} + {ANSWER1} > 0 else 'no'\")\n",
            "FINAL_RESULT=RESULT(var=ANSWER2)\n"
          ]
        }
      ]
    }
  ]
}
